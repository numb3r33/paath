{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "DATA_PATH  = Path('data').parent / 'drive' / 'MyDrive' / 'paathai' / 'data'\n",
    "TRAIN_PCT  = 0.9\n",
    "\n",
    "def load_data():\n",
    "  with open(DATA_PATH / 'input.txt', 'r') as f:\n",
    "      data = f.read()\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "def load_sample_data():\n",
    "  return \"Natural Language Processing and Machine Learning is very interesting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize\n",
    "\n",
    "- Should we strip extra whitespace from the text?\n",
    "- Should we make every character small case?\n",
    "- What should be the special padding char used to make sure equal batches are generated from the corpus?\n",
    "- What should be the index assigned to the special char?\n",
    "- Create char2index dict \n",
    "- Create index2char dict\n",
    "- How to create one_hot_vector of each individual char encoding using only numpy?\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "- First we convert string 2 index\n",
    "- Then we convert index 2 one hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "data = load_data()\n",
    "# data = load_sample_data()\n",
    "\n",
    "n    = len(data)\n",
    "\n",
    "train_data = data[:int(n*TRAIN_PCT)]\n",
    "val_data   = data[int(n*TRAIN_PCT):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter(data)\n",
    "SPECIAL_CHAR = '`'\n",
    "\n",
    "def make_vocab(c, min_freq=3):\n",
    "  vocab = [o for o, v in c.most_common() if v >= min_freq]\n",
    "  return  [SPECIAL_CHAR] + vocab\n",
    "\n",
    "def w2i(vocab): return {w:i for i,w in enumerate(vocab)}\n",
    "def i2w(vocab): return {i:w for i,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'F': 1797,\n",
       "         'i': 45537,\n",
       "         'r': 48889,\n",
       "         's': 49696,\n",
       "         't': 67009,\n",
       "         ' ': 169892,\n",
       "         'C': 3820,\n",
       "         'z': 356,\n",
       "         'e': 94611,\n",
       "         'n': 48529,\n",
       "         ':': 10316,\n",
       "         '\\n': 40000,\n",
       "         'B': 2761,\n",
       "         'f': 15770,\n",
       "         'o': 65798,\n",
       "         'w': 17585,\n",
       "         'p': 10808,\n",
       "         'c': 15623,\n",
       "         'd': 31358,\n",
       "         'a': 55507,\n",
       "         'y': 20448,\n",
       "         'u': 26584,\n",
       "         'h': 51310,\n",
       "         ',': 19846,\n",
       "         'm': 22243,\n",
       "         'k': 7088,\n",
       "         '.': 7885,\n",
       "         'A': 7819,\n",
       "         'l': 33339,\n",
       "         'S': 4523,\n",
       "         'Y': 1718,\n",
       "         'v': 7793,\n",
       "         '?': 2462,\n",
       "         'R': 4869,\n",
       "         'M': 2840,\n",
       "         'W': 3530,\n",
       "         \"'\": 6187,\n",
       "         'L': 3876,\n",
       "         'I': 11832,\n",
       "         'N': 5079,\n",
       "         'g': 13356,\n",
       "         ';': 3628,\n",
       "         'b': 11321,\n",
       "         '!': 2172,\n",
       "         'O': 5481,\n",
       "         'j': 628,\n",
       "         'V': 798,\n",
       "         '-': 1897,\n",
       "         'T': 7015,\n",
       "         'H': 3068,\n",
       "         'E': 6041,\n",
       "         'U': 3313,\n",
       "         'D': 2089,\n",
       "         'P': 1641,\n",
       "         'q': 609,\n",
       "         'x': 529,\n",
       "         'J': 320,\n",
       "         'G': 2399,\n",
       "         'K': 1584,\n",
       "         'Q': 231,\n",
       "         '&': 3,\n",
       "         'Z': 198,\n",
       "         'X': 112,\n",
       "         '3': 27,\n",
       "         '$': 1})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "vocab = make_vocab(c)\n",
    "wi = w2i(vocab)\n",
    "iw = i2w(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 65)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "idx = sorted(wi.values())\n",
    "eye = np.eye(max(idx) + 1)\n",
    "\n",
    "aa = eye[idx]\n",
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'`': 0,\n",
       " ' ': 1,\n",
       " 'e': 2,\n",
       " 't': 3,\n",
       " 'o': 4,\n",
       " 'a': 5,\n",
       " 'h': 6,\n",
       " 's': 7,\n",
       " 'r': 8,\n",
       " 'n': 9,\n",
       " 'i': 10,\n",
       " '\\n': 11,\n",
       " 'l': 12,\n",
       " 'd': 13,\n",
       " 'u': 14,\n",
       " 'm': 15,\n",
       " 'y': 16,\n",
       " ',': 17,\n",
       " 'w': 18,\n",
       " 'f': 19,\n",
       " 'c': 20,\n",
       " 'g': 21,\n",
       " 'I': 22,\n",
       " 'b': 23,\n",
       " 'p': 24,\n",
       " ':': 25,\n",
       " '.': 26,\n",
       " 'A': 27,\n",
       " 'v': 28,\n",
       " 'k': 29,\n",
       " 'T': 30,\n",
       " \"'\": 31,\n",
       " 'E': 32,\n",
       " 'O': 33,\n",
       " 'N': 34,\n",
       " 'R': 35,\n",
       " 'S': 36,\n",
       " 'L': 37,\n",
       " 'C': 38,\n",
       " ';': 39,\n",
       " 'W': 40,\n",
       " 'U': 41,\n",
       " 'H': 42,\n",
       " 'M': 43,\n",
       " 'B': 44,\n",
       " '?': 45,\n",
       " 'G': 46,\n",
       " '!': 47,\n",
       " 'D': 48,\n",
       " '-': 49,\n",
       " 'F': 50,\n",
       " 'Y': 51,\n",
       " 'P': 52,\n",
       " 'K': 53,\n",
       " 'V': 54,\n",
       " 'j': 55,\n",
       " 'q': 56,\n",
       " 'x': 57,\n",
       " 'z': 58,\n",
       " 'J': 59,\n",
       " 'Q': 60,\n",
       " 'Z': 61,\n",
       " 'X': 62,\n",
       " '3': 63,\n",
       " '&': 64}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 10, 8, 7, 3, 1, 38, 10, 3, 10]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "cwi = [wi[o_] for o_ in data if o_ in wi]\n",
    "cwi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citi'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "''.join([iw[o_] for o_ in cwi[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "class Dataset():\n",
    "  def __init__(self, x, vocab, cw): \n",
    "    self.x = x\n",
    "    self.vocab = vocab\n",
    "    self.cw = cw \n",
    "\n",
    "    self._setup()\n",
    "    self._numericalize()\n",
    "    self._create_items()\n",
    "\n",
    "  def _setup(self):\n",
    "    self.w2i = {w:i for i,w in enumerate(self.vocab)}\n",
    "    self.i2w = {i:w for i,w in enumerate(self.vocab)}\n",
    "\n",
    "  def _numericalize(self):\n",
    "    x = SPECIAL_CHAR*self.cw+self.x+SPECIAL_CHAR*self.cw\n",
    "    self.nt = [self.w2i[o_] for o_ in x if o_ in self.w2i]\n",
    "\n",
    "  def _create_items(self):\n",
    "    \n",
    "    self.contexts = []\n",
    "    self.targets  = []\n",
    "\n",
    "    for i in range(0+self.cw, len(self.nt)-self.cw):\n",
    "      lw = self.nt[max(0, i-self.cw):i]\n",
    "      rw = self.nt[i+1:min(i+self.cw+1, len(self.nt))]\n",
    "      context = lw + rw\n",
    "      target  = self.nt[i]\n",
    "\n",
    "      self.contexts.append(context)\n",
    "      self.targets.append(target)\n",
    "\n",
    "  def __len__(self): return len(self.contexts)\n",
    "  def __getitem__(self, i): return tensor(self.contexts[i]), tensor(self.targets[i], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  0, 10,  8],\n",
       "         [ 0, 50,  8,  7]]),\n",
       " tensor([50, 10]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "c     = Counter(data)\n",
    "vocab = make_vocab(c)\n",
    "\n",
    "dset = Dataset(data, vocab, cw=2)\n",
    "dset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "train_ds, valid_ds = Dataset(train_data, vocab, cw=2), Dataset(val_data, vocab, cw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0, 10,  8],\n",
       "        [ 0, 50,  8,  7],\n",
       "        [50, 10,  7,  3],\n",
       "        [10,  8,  3,  1],\n",
       "        [ 8,  7,  1, 38]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "xb, yb = train_ds[0:5]\n",
    "assert len(xb)==5\n",
    "\n",
    "xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "- Batch Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "class DataLoader():\n",
    "  def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "  def __iter__(self):\n",
    "    for i in range(0, len(self.ds), self.bs):\n",
    "      yield self.ds[i:i+self.bs]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "bs       = 64\n",
    "train_dl = DataLoader(train_ds, bs=bs)\n",
    "valid_dl = DataLoader(valid_ds, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "xb, yb = next(iter(train_dl))\n",
    "len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "class CBOW(Module):\n",
    "  def __init__(self, vocab_size, embed_size):\n",
    "    self.embed_size = embed_size\n",
    "\n",
    "    self.embed  = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embed_size)\n",
    "    self.linear = nn.Linear(self.embed_size, vocab_size)\n",
    "    self.layers = [self.embed, self.linear]\n",
    "\n",
    "  def forward(self, x):\n",
    "    out    = torch.sum(self.embed(x), dim=1)\n",
    "    logits = self.linear(out)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 65])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "model = CBOW(vocab_size=len(vocab), embed_size=2)\n",
    "ret   = model(xb)\n",
    "\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.1967,  1.7395,  1.7965, -3.3907,  3.4688,  0.0461, -0.4433,  0.1141,\n",
       "         -3.1695,  0.1360], grad_fn=<SelectBackward0>),\n",
       " torch.Size([2, 10]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "xb, yb = next(iter(train_dl))\n",
    "preds  = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1907, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "lr     = 0.1\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.660706996917725\n",
      "3.1420717239379883\n",
      "3.136988878250122\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  for i, (xb, yb) in enumerate(train_dl):\n",
    "    preds = model(xb)\n",
    "    loss  = loss_func(preds, yb)\n",
    "    loss.backward()\n",
    "\n",
    "    if i==0: print(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for l in model.layers:\n",
    "        if hasattr(l, 'weight'):\n",
    "          l.weight -= l.weight.grad * lr\n",
    "          l.weight.grad.zero_()\n",
    "          \n",
    "          if hasattr(l, 'bias'):\n",
    "            l.bias   -= l.bias.grad * lr\n",
    "            l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.678817962315194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "!nbdev_export\n",
    "#import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
